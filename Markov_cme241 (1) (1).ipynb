{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments from january 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write out the MP/MRP definitions and MRP Value Function definition (in LaTeX) in your own style/notation (so you really internalize these concepts) $\\Done$\n",
    "- Think about the data structures/class design (in Python 3) to represent MP/MRP and implement them with clear type declarations $\\Not\\;\\done$ -- I will try to see how this works later. For now I am sticking with my undefined data structures.\n",
    "- Remember your data structure/code design must resemble the Mathematical/notational formalism as much as possible $\\Not\\;\\Done$\n",
    "- Specifically the data structure/code design of MRP should be incremental (and not independent) to that of MP $\\Done$\n",
    "- Separately implement the $r(s,s')$ and the $\\mathcal{R}(s) = \\sum_{s'} p(s,s') \\cdot r(s,s')$ definitions of MRP $\\Done$\n",
    "- Write code to convert/cast the r(s,s') definition of MRP to the R(s) definition of MRP (put some thought into code  design here) $\\Done$\n",
    "- Write code to generate the stationary distribution for an MP $\\Done$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we have some definitions. The value function $v(s)$ is defines as \n",
    "$$v(s)=\\mathbb{E}\\Big[\\sum_{i=0}^{\\infty}\\gamma^iR_{t+i+1}\\Big|S_t=s\\Big],$$\n",
    "where $R_i$ is the reward at time $i$ and $\\gamma$ is the discount factor. Furthermore, the reward function $\\mathcal{R}(s)$ is defined as $$\\mathcal{R}(s)=\\mathbb{E}[R_{t}|S_{t-1}=s].$$\n",
    "Following the notations from Sutton's book we also have the function $r(s,s')$ such that \n",
    "$$r(s,s')=\\mathbb{E}[R_t|S_{t-1}=s\\;\\cap\\;S_t=s']$$ and the function $$p(s,s')=\\mathbb{P}(S_t=s'|S_{t-1}=s).$$\n",
    "Thus we see that \n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\mathcal{R}(s)&=\\mathbb{E}[R_{t}|S_{t-1}=s]\\\\\n",
    "        &=\\sum_{s'}R_t(\\{\\texttt{reward after state }\n",
    "        s'\\})\\mathbb{P}(S_t=s'|S_{t-1}=s)\\\\\n",
    "        &=\\sum_{s'}\\mathbb{E}[R_t|S_{t-1}=s\\;\\cap\\;S_t=s']\\mathbb{P}(S_t=s'|S_{t-1}=s)\\\\\n",
    "        &=\\sum_{s'}r(s,s')p(s,s').\n",
    "    \\end{split}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necesarry packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov Process** - defined by states and probability matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP(object):\n",
    "    def __init__(self, ProbDistribution,States:list=None, print_info:bool=False, Name:str=\"Nameless\"):\n",
    "        self.Name=Name\n",
    "        if States == None:\n",
    "            self.States = [i for i in range(len(ProbDistribution))]\n",
    "        else:\n",
    "            self.States = States\n",
    "        self.ProbDistribution = ProbDistribution\n",
    "        self.print_info = print_info\n",
    "        if print_info == True:\n",
    "            print(\"The Markov process\", Name, \"has been created. It has\", len(States), \"states.\")\n",
    "        assert len(self.States) == len(self.ProbDistribution)\n",
    "    \n",
    "    def Generate_Stationary_Dist(self):\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(self.ProbDistribution.T)\n",
    "        stat_dist=np.zeros((len(self.States),1))\n",
    "        for i in range(len(eigenvalues)):\n",
    "            if abs(eigenvalues[i]-1) < 1e-8:\n",
    "                stat_dist = stat_dist + eigenvectors[:,i].reshape(len(self.States),1)\n",
    "        return((stat_dist/np.sum(stat_dist)).reshape(1,len(self.States)))\n",
    "    \n",
    "    def Simulate(self,steps:int=10,start=None,print_text=False):\n",
    "        #steps=len(self.States)\n",
    "        if start == None:\n",
    "            start = self.States[0]\n",
    "        path = [start]\n",
    "        current_activity = start\n",
    "        i=0\n",
    "        while i < steps:\n",
    "            for j in range(len(self.States)):\n",
    "                if current_activity == self.States[j]:\n",
    "                    RV = np.random.choice(self.States,replace=True,p=self.ProbDistribution[j])\n",
    "                    for k in range(len(self.States)):\n",
    "                        if RV == self.States[k]:\n",
    "                            if self.ProbDistribution[j][k] == 1 and k == j:\n",
    "                                if print_text == True:\n",
    "                                    print(\"The procces reached the termination state\", \"'\",self.States[j],\"'\", \"after\", i, \"steps.\")\n",
    "                                i = steps\n",
    "                                break\n",
    "                            path.append(self.States[k])\n",
    "                            current_activity = self.States[k]\n",
    "                            break\n",
    "                    break\n",
    "            i += 1\n",
    "        if print_text == True:\n",
    "            print(\"The path was:\", path)\n",
    "        return(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we can define a Markov Reward Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRP(MP):\n",
    "    def __init__(self,ProbDistribution,Rewards,States:list=None,gamma=1,ABC=\"A\"):\n",
    "        MP.__init__(self,ProbDistribution,States)\n",
    "        self.ABC=ABC\n",
    "        if self.ABC == \"A\":\n",
    "            assert Rewards.shape == (len(ProbDistribution), 1)\n",
    "            self.Rewards=Rewards\n",
    "        elif self.ABC == \"B\":\n",
    "            assert Rewards.shape == (len(ProbDistribution), len(ProbDistribution))\n",
    "            self.Rewards=np.diag(np.dot(self.ProbDistribution,Rewards)).reshape(len(States),1)\n",
    "        elif self.ABC == \"C\":\n",
    "            print(\"Please use definition A or B\") \n",
    "            #i.e. no idea what to do here\n",
    "        self.gamma=gamma\n",
    "        \n",
    "    def Get_Expected_Reward_one_state(self,start=None):\n",
    "        if start == None:\n",
    "            return(np.dot(self.ProbDistribution,self.Rewards))\n",
    "        else:\n",
    "            for i in range(len(self.States)):\n",
    "                if self.States[i]==start:\n",
    "                    return((np.dot(self.ProbDistribution,self.Rewards))[i])\n",
    "                \n",
    "    def Get_Value_Function(self):\n",
    "        if np.linalg.det(self.ProbDistribution) > 1e-5:\n",
    "            R=np.dot(self.ProbDistribution,self.Rewards)\n",
    "            inverse=np.linalg.inv(np.identity(len(self.States))-self.gamma*self.ProbDistribution)\n",
    "            return(np.dot(inverse,R))\n",
    "        else:\n",
    "            print(\"Determinant Zero -- wait for simulation\")\n",
    "            VF=np.zeros((len(self.States),1))\n",
    "            for item in self.States:\n",
    "                n=4000\n",
    "                test=0\n",
    "                for i in range(n):\n",
    "                    test+=self.Simulate_Rewards(n,item)\n",
    "                VF[self.States.index(item)]=test/n\n",
    "            return(VF)\n",
    "    \n",
    "    def rss_To_RS(self):\n",
    "        assert self.ABC==\"B\"\n",
    "        return(self.Rewards)\n",
    "        \n",
    "    def Simulate_Rewards(self,steps:int=10,start=None,print_text=False):\n",
    "        if isinstance(start,(int,float)) == True:\n",
    "            start=self.States[int(start)]\n",
    "        path = self.Simulate(steps,start,print_text)\n",
    "        accumulated_reward=0\n",
    "        i=0\n",
    "        for item in path:\n",
    "            path_ind=self.States.index(item)\n",
    "            accumulated_reward=accumulated_reward+self.Rewards[path_ind]*self.gamma**i\n",
    "            i+=1\n",
    "        if print_text == True:\n",
    "            print('The path Resulted in a value of ', float(accumulated_reward))\n",
    "        return(float(accumulated_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can now test how the code runs** - we use the example from the lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([[0,0.5,0,0,0,0.5,0],\n",
    "              [0,0,0.8,0,0,0,0.2],\n",
    "              [0,0,0,0.6,0.4,0,0],\n",
    "              [0,0,0,0,0,0,1],\n",
    "              [0.2,0.4,0.4,0,0,0,0],\n",
    "              [0.1,0,0,0,0,0.9,0],\n",
    "              [0,0,0,0,0,0,1]])\n",
    "S=[\"C1\",\"C2\",\"C3\",\"Pass\",\"Pub\",\"FB\",\"Sleep\"]\n",
    "R=np.array([-2,-2,-2,10,1,-1,0]).reshape(len(P),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant Zero -- wait for simulation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-12.6332],\n",
       "       [  1.6798],\n",
       "       [  4.4716],\n",
       "       [ 10.    ],\n",
       "       [  0.4838],\n",
       "       [-22.9384],\n",
       "       [  0.    ]])"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_R_P=MRP(P,R,S)\n",
    "M_R_P.Get_Value_Function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments from January 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write the Bellman equation for MRP Value Function and code to calculate MRP Value Function (based on Matrix inversion method you learnt in this lecture) -- $\\Done $\n",
    "- Write out the MDP definition, Policy definition and MDP Value Function definition (in LaTeX) in your own style/notation (so you really internalize these concepts) \n",
    "- Think about the data structure/class design (in Python 3) to represent MDP, Policy, Value Function, and implement them with clear type definitions $\\Not \\;\\done$\n",
    "- The data structure/code design of MDP should be incremental (and not independent) to that of MRP\n",
    "- Separately implement the $r(s,s',a)$ and $R(s,a) = \\sum_{s'} p(s,s',a) * r(s,s',a)$ definitions of MDP $\\Not \\;\\done$ -- again, not really sure what to do here....\n",
    "- Write code to convert/cast the $r(s,s',a)$ definition of MDP to the $R(s,a)$ definition of MDP (put some thought into code design here) $\\Not\\;\\Done$ -- also no idea\n",
    "- Write code to create a MRP given a MDP and a Policy $\\Done$\n",
    "- Write out all 8 MDP Bellman Equations and also the transformation from Optimal Action-Value function to Optimal Policy (in LaTeX) $\\Done$\n",
    "                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MDP is a MRP with actions $A$ at some states of the MRP. A policy $\\pi(a|s)$ is the probability that the agent does action $a$ in in state $s$. The value function of an MDP is defined in the same way as a value function for an MRP, but taking the action into account as well. We denote the value function for a given policy $\\pi$ as $$q_\\pi(s,a)=\\mathbb{E}\\Big[\\sum_{i=0}^{\\infty}\\gamma^iR_{t+i+1}\\Big|S_t=s\\;\\cap\\;A_t=a\\Big].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP(MRP):\n",
    "    def __init__(self,ProbDistribution,Rewards,States:list=None,gamma=1,ABC=\"A\",Action_States=None,Policy=None,Feasible_Steps=None):\n",
    "        MRP.__init__(self,ProbDistribution,Rewards,States,gamma,ABC)\n",
    "        self.Policy=Policy\n",
    "        self.Action_States=Action_States\n",
    "        self.Feasible_Steps=Feasible_Steps\n",
    "        if np.all(self.Policy) != None and self.Action_States != None:\n",
    "            for item in self.Action_States:\n",
    "                if isinstance(item, str) == True:\n",
    "                    self.ProbDistribution[self.States.index(item)] = self.Policy[self.States.index(item)]\n",
    "                elif isinstance(item, (float,int)) == True:\n",
    "                    self.ProbDistribution[int(item)] = self.Policy[int(item)]\n",
    "        elif np.all(self.Policy) == None and self.Action_States != None and np.all(self.Feasible_Steps) !=None:\n",
    "            k=0\n",
    "            for item in self.Action_States:\n",
    "                if isinstance(item, str) == True:\n",
    "                    i=self.States.index(item)\n",
    "                elif isinstance(item,(float,int)) == True:\n",
    "                    i=item\n",
    "                VF=self.Get_Value_Function()\n",
    "                for j in range(len(VF)):\n",
    "                    if self.Feasible_Steps[k,j] == 0:\n",
    "                        VF[j]=0\n",
    "                ind = np.unravel_index(np.argmax(VF, axis=None), VF.shape)\n",
    "                new_dist=np.zeros((1,len(self.ProbDistribution)))\n",
    "                new_dist[0,ind[0]] = 1\n",
    "                self.ProbDistribution[i] = new_dist\n",
    "                k+=1\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code takes either a policy or a set of possible directions. If the we input the feasible directions, the code finds the policy that gives the highest value function. Code belows shows the example from the lecture where you can go to either C1, C2 or C3 from the Pub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant Zero -- wait for simulation\n"
     ]
    }
   ],
   "source": [
    "M_D_P=MDP(P,R,S,1,\"A\",[\"Pub\"],None,np.array([1,1,1,0,0,0,0]).reshape(1,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant Zero -- wait for simulation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-10.086 ],\n",
       "       [  3.792 ],\n",
       "       [  7.3315],\n",
       "       [ 10.    ],\n",
       "       [  8.3395],\n",
       "       [-20.209 ],\n",
       "       [  0.    ]])"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_D_P.Get_Value_Function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the value function has now increased in all steps, as the process always goes to C3 from Pub instead. Below you can se that this has also changed in the transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.5, 0. , 0. , 0. , 0.5, 0. ],\n",
       "       [0. , 0. , 0.8, 0. , 0. , 0. , 0.2],\n",
       "       [0. , 0. , 0. , 0.6, 0.4, 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0. , 1. ],\n",
       "       [0. , 0. , 1. , 0. , 0. , 0. , 0. ],\n",
       "       [0.1, 0. , 0. , 0. , 0. , 0.9, 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0. , 1. ]])"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
