{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Markov chain that also implements a reward process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created a simple coded that simulates any markov process and returns the accumulated reward if a reward is stated. This is not the the saught-after code but intuitively it worked pretty well for me to get a good understanding of the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MarkovSimulation(changes,P,states,start,discount=None,reward=None,print_text=False):\n",
    "    import numpy as np\n",
    "    import random\n",
    "    # Changes = amount of times the process will change (integer)\n",
    "    # P = transition matrix (matrix)\n",
    "    # states = possible states (list)\n",
    "    # reward = reward at each state (list of numerics values)\n",
    "    # start = start value in transition matrix (string)\n",
    "    # discount = discount factor between each state (numeric)\n",
    "    path=[start]\n",
    "    current_activity = start\n",
    "    if reward != None:\n",
    "        r=0\n",
    "        for text in states:\n",
    "            if start in text:\n",
    "                accumulated_reward = reward[r]\n",
    "            r += 1\n",
    "        if discount == None:\n",
    "            discount = 1\n",
    "    i = 0\n",
    "    while i < changes:\n",
    "        for j in range(len(P)):\n",
    "            if current_activity == states[j]:\n",
    "                #print(\"j:\", j ) -- this can be used to debugging\n",
    "                RV = np.random.choice(states,replace=True,p=P[j])\n",
    "                for k in range(len(P)):\n",
    "                    if RV == states[k]:\n",
    "                        if P[j][k] == 1 and k == j:\n",
    "                            if print_text == True:\n",
    "                                print(\"The procces reached the termination state\", \"'\",states[j],\"'\", \"after\", i, \"steps.\")\n",
    "                            i = changes\n",
    "                            break\n",
    "                        path.append(states[k])\n",
    "                        current_activity = states[k]\n",
    "                        if reward != None:\n",
    "                            accumulated_reward += reward[k]*discount**(i+1)\n",
    "                        #print(\"k:\", k) -- this can be used to debugging\n",
    "                        break\n",
    "                break\n",
    "        i += 1\n",
    "    if print_text == True:\n",
    "        print(\"The path was:\", path)\n",
    "        if reward != None:\n",
    "            print(\"The procces resulted in a reward of: \", accumulated_reward, \".\")\n",
    "    if reward != None:\n",
    "        return(path,accumulated_reward)\n",
    "    else:\n",
    "        return(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this code I can either define a transition matrix and it's states or both of those and a reward. E.g:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "States =[['C1','C2','C3','Pass','Pub','FB','Sleep'],[-2,-2,-2,10,1,-1,0]]\n",
    "P = [[0,0.5,0,0,0,0.5,0],\n",
    "         [0,0,0.8,0,0,0,0.2],\n",
    "         [0,0,0,0.6,0.4,0,0],\n",
    "         [0,0,0,0,0,0,1],\n",
    "         [0.2,0.4,0.4,0,0,0,0],\n",
    "         [0.1,0,0,0,0,0.9,0],\n",
    "         [0,0,0,0,0,0,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting at C1, we can then simulate one path with maximum 50 steps as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The procces reached the termination state ' Sleep ' after 14 steps.\n",
      "The path was: ['C1', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'FB', 'C1', 'C2', 'C3', 'Pub', 'C1', 'C2', 'Sleep']\n",
      "The procces resulted in a reward of:  -18 .\n"
     ]
    }
   ],
   "source": [
    "path,R=MarkovSimulation(50,P,States[0],States[0][0],1,States[1],True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also simulate the chain several times to get an estimation of $V(s)$. In this case we start at \"FB\" and set $\\gamma=0.9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.58968488758998\n"
     ]
    }
   ],
   "source": [
    "n=10000\n",
    "EV=0\n",
    "for i in range(n):\n",
    "    path,R=MarkovSimulation(n,P,States[0],States[0][5],0.9,States[1])\n",
    "    EV += R\n",
    "print(EV/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example finished"
   ]
  },
  
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
