{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See goals below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write out the MP/MRP definitions and MRP Value Function definition (in LaTeX) in your own style/notation (so you really internalize these concepts)\n",
    "- Think about the data structures/class design (in Python 3) to represent MP/MRP and implement them with clear type declarations\n",
    "- Remember your data structure/code design must resemble the Mathematical/notational formalism as much as possible\n",
    "- Specifically the data structure/code design of MRP should be incremental (and not independent) to that of MP\n",
    "- Separately implement the $r(s,s')$ and the $\\mathcal{R}(s) = \\sum_{s'} p(s,s') \\cdot r(s,s')$ definitions of MRP\n",
    "- Write code to convert/cast the r(s,s') definition of MRP to the R(s) definition of MRP (put some thought into code design here)\n",
    "- Write code to generate the stationary distribution for an MP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we have some definitions. The value function $v(s)$ is defines as \n",
    "$$v(s)=\\mathbb{E}\\Big[\\sum_{i=0}^{\\infty}\\gamma^iR_{t+i+1}\\Big|S_t=s\\Big],$$\n",
    "where $R_i$ is the reward at time $i$ and $\\gamma$ is the discount factor. Furthermore, the reward function $\\mathcal{R}(s)$ is defined as $$\\mathcal{R}(s)=\\mathbb{E}[R_{t}|S_{t-1}=s].$$\n",
    "Following the notations from Sutton's book we also have the function $r(s,s')$ such that \n",
    "$$r(s,s')=\\mathbb{E}[R_t|S_{t-1}=s\\;\\cap\\;S_t=s']$$ and the function $$p(s,s')=\\mathbb{P}(S_t=s'|S_{t-1}=s).$$\n",
    "Thus we see that \n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\mathcal{R}(s)&=\\mathbb{E}[R_{t}|S_{t-1}=s]\\\\\n",
    "        &=\\sum_{s'}R_t(\\{\\texttt{reward after state }\n",
    "        s'\\})\\mathbb{P}(S_t=s'|S_{t-1}=s)\\\\\n",
    "        &=\\sum_{s'}\\mathbb{E}[R_t|S_{t-1}=s\\;\\cap\\;S_t=s']\\mathbb{P}(S_t=s'|S_{t-1}=s)\\\\\n",
    "        &=\\sum_{s'}r(s,s')p(s,s').\n",
    "    \\end{split}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necesarry packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we define a Markov Process (MP) as a class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP(object):\n",
    "    def __init__(self, States:list, ProbDistribution, print_info:bool=False, Name:str=\"Nameless\"):\n",
    "        self.Name=Name\n",
    "        self.States = States\n",
    "        self.ProbDistribution = ProbDistribution\n",
    "        self.print_info = print_info\n",
    "        if print_info == True:\n",
    "            print(\"The Markov process\", Name, \"has been created. It has\", len(States), \"states.\")\n",
    "    \n",
    "    def Generate_Stationary_Dist(self):\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(self.ProbDistribution.T)\n",
    "        stat_dist=np.zeros((len(self.States),1))\n",
    "        for i in range(len(eigenvalues)):\n",
    "            if abs(eigenvalues[i]-1) < 1e-8:\n",
    "                stat_dist = stat_dist + eigenvectors[:,i].reshape(len(self.States),1)\n",
    "        return((stat_dist/np.sum(stat_dist)).reshape(1,len(self.States)))\n",
    "    \n",
    "    def Simulate(self,steps:int,start=None,print_text=False):\n",
    "        #steps=len(self.States)\n",
    "        if start == None:\n",
    "            start = self.States[0]\n",
    "        path = [start]\n",
    "        current_activity = start\n",
    "        i=0\n",
    "        while i < steps:\n",
    "            for j in range(len(self.States)):\n",
    "                if current_activity == self.States[j]:\n",
    "                    RV = np.random.choice(self.States,replace=True,p=self.ProbDistribution[j])\n",
    "                    for k in range(len(self.States)):\n",
    "                        if RV == self.States[k]:\n",
    "                            if self.ProbDistribution[j][k] == 1 and k == j:\n",
    "                                if print_text == True:\n",
    "                                    print(\"The procces reached the termination state\", \"'\",self.States[j],\"'\", \"after\", i, \"steps.\")\n",
    "                                i = steps\n",
    "                                break\n",
    "                            path.append(self.States[k])\n",
    "                            current_activity = self.States[k]\n",
    "                            break\n",
    "                    break\n",
    "            i += 1\n",
    "        if print_text == True:\n",
    "            print(\"The path was:\", path)\n",
    "        return(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class MP is defined by it's states and their probabilites. Now we can test the two functions implemented in the Markov process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_MP = MP([\"Sleep\",\"Wake up\",\"Eat\"],np.array([1/2,1/4,1/4,0,2/3,1/3,1/3,1/3,1/3]).reshape(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21052632, 0.47368421, 0.31578947]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_MP.Generate_Stationary_Dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path was: ['Sleep', 'Sleep', 'Sleep', 'Wake up', 'Eat', 'Eat']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Sleep', 'Sleep', 'Sleep', 'Wake up', 'Eat', 'Eat']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_MP.Simulate(5,\"Sleep\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we can define a Markov Reward Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRP(MP):\n",
    "    def __init__(self,States,ProbDistribution,Rewards,gamma):\n",
    "        MP.__init__(self, States, ProbDistribution)\n",
    "        self.Rewards=Rewards\n",
    "        self.gamma=gamma\n",
    "        \n",
    "    def Get_Expected_Reward_one_state(self,start=None):\n",
    "        if start == None:\n",
    "            return(np.dot(self.ProbDistribution,self.Rewards))\n",
    "        else:\n",
    "            for i in range(len(self.States)):\n",
    "                if self.States[i]==start:\n",
    "                    return((np.dot(self.ProbDistribution,self.Rewards))[i])\n",
    "                \n",
    "    def Get_Value_Function(self):\n",
    "        R=np.dot(self.ProbDistribution,self.Rewards)\n",
    "        inverse=np.linalg.inv(np.identity(len(self.States))-self.gamma*self.ProbDistribution)\n",
    "        return(np.dot(inverse,R))\n",
    "    \n",
    "    def rss_to_RS(self):\n",
    "        RS=self.Get_Expected_Reward_one_state()\n",
    "        return(RS)\n",
    "        #I do not at all understand what I am supposed to do here \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Test_MRP = MRP(Test_MP.States,Test_MP.ProbDistribution,np.array([1,2,3]).reshape(3,1),0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[209.91591902],\n",
       "       [210.96027683],\n",
       "       [210.28230542]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_MRP.Get_Value_Function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
